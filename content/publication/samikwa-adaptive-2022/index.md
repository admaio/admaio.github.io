---
title: Adaptive Early Exit of Computation for Energy-Efficient and Low-Latency Machine
  Learning over IoT Networks
authors:
- Eric Samikwa
- Antonio Di Maio
- Torsten Braun
date: '2022-01-01'
publishDate: '2024-05-25T00:58:24.426045Z'
publication_types:
- paper-conference
publication: '*2022 IEEE 19th Annual Consumer Communications & Networking Conference
  (CCNC)*'
doi: 10.1109/CCNC49033.2022.9700550
abstract: Large Machine Learning (ML) models require considerable computing resources
  and raise challenges for integrating them with the decentralized operation of heterogeneous
  and resource-constrained Internet of Things (IoT) devices. Running ML tasks on the
  cloud can introduce network delay, throughput, and privacy concerns, whereas running
  ML tasks on IoT devices is penalized by their constrained resources. For this reason,
  recent research proposed cooperative execution of ML tasks over IoT networks but
  disregarded resource variability and the IoT devices’ energy constraints simultaneously.
  In this paper, we propose Early Exit of Computation (EEoC), an adaptive, energy-efficient,
  low-latency inference scheme over IoT networks. EEoC adaptively distributes the
  inference computation load between the IoT device and the edge server, based on
  estimated communication and computation resources, to jointly minimize prediction
  latency and energy consumption. We evaluate our solution’s latency and energy profile
  on a real testbed running two widely used neural networks. Results show that EEoC
  can reduce latency and energy consumption up to 24.6% and 46.5%, respectively, compared
  to other state-of-the-art solutions without sacrificing accuracy.
tags:
- Computational modeling
- Throughput
- Energy consumption
- Internet of Things
- Servers
- Edge Computing
- Adaptive systems
- Machine learning
- DNN partitioning
- Early Exit of Computation
- Energy efficiency
- Machine Learning
---
